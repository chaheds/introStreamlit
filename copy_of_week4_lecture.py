# -*- coding: utf-8 -*-
"""Copy of week4_lecture.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uB8UBp5AIliZSY3hjMqb-g2lHQ2-Usk5
"""

# Commented out IPython magic to ensure Python compatibility.
# import packages
import pandas as pd
import numpy as np
import seaborn as sns

import matplotlib.pyplot as plt
import matplotlib.mlab as mlab
import matplotlib
plt.style.use('ggplot')
from matplotlib.pyplot import figure

# %matplotlib inline
matplotlib.rcParams['figure.figsize'] = (12,8)

pd.options.mode.chained_assignment = None

#Accessing local file system using Python code.
#This step requires you to first import the files module from the google.colab library
from google.colab import files

#use the upload method of the files object.
#Running this opens the File Upload dialog window
#Select the file(s) you wish to upload, and then wait for the upload to complete
uploaded = files.upload()

# read the data
df = pd.read_csv('housing.csv')

#shape and data types of the data
print(df.shape)
print(df.dtypes)

#select numeric columns
df_numeric = df.select_dtypes(include = [np.number])
numeric_cols = df_numeric.columns.values
print(numeric_cols)

#select non numeric columns
df_non_numeric = df.select_dtypes(exclude=[np.number])
non_numeric_cols = df_non_numeric.columns.values
print(non_numeric_cols)

# print summary of all non-numeric columns
df[non_numeric_cols].info()

"""#Missing data

##Identifying missing data


"""

# visualise missing data via heatmap
#this technique can be used when we have a small number of variables
cols = df.columns[:30] # display first 30 columns
sns.heatmap(df[cols].isnull())

#using different colours for the heatmap
plt.figure(figsize=(10,8))
cols = df.columns[:30] # display first 30 columns
colours = ['#3D59AB','#FF7256'] #Cobalt for non-missing, coral1 for missing
#for more colour codes, check: https://www.webucator.com/article/python-color-constants-module/
sns.heatmap(df[cols].isnull(), cmap=sns.color_palette(colours))

#create missing data heatmap using missingno library
import missingno as msno
msno.matrix(df.iloc[:,:30])

#missing data percentage list
# if it is a large dataset and the visualisation takes toolong, percentage list is preferred.
# percentage of missing
##option1
##for col in df.columns:
  ##pct_missing = np.mean(df[col].isnull())
  ##print('{} - {}%'.format(col, round(pct_missing*100)))

##option2
##missing_pct = round(df.isnull().sum()/len(df)*100,1)
##print(missing_pct)

#columns and numbers of missing values
num_missing = df.isna().sum()
num_missing[:13]

#percentages of missing values by column
pct_missing = df.isna().mean()
pct_missing[:13]

#missing data histogram
#summarising the missing data by rows via a histogram

missing_by_row = df.isna().sum(axis='columns')
missing_by_row.hist(bins=50)

"""##Tackling missing data"""

# Drop the entire column with missing data
#pct_missing stores the list of columns and their missing data percentages
# Let's identify the columns with with over 30% missing data?
pct_missing[pct_missing > 0.3]

pct_missing[pct_missing <= 0.3]

#if columns with more than 30% missing values are not useful for our analysis, we can leave them out.
#we can copy columns with less than or equal to 30% missing to a new data frame.
#the line below is equivalent to df.drop(columns=pct_missing[pct_missing > 0.3].index)
df_less_missing_cols = df.loc[:, pct_missing <= 0.3].copy()
df_less_missing_cols.shape

df.shape

#Drop entire row with missing data
#Create a new dataset to only keep observations with less than 35 missing columns
df_less_missing_rows = df.dropna(axis='index', thresh=292-35+1)
df_less_missing_rows.shape

"""##Impute missing data with statistics"""

#Inpute the numeric columns with their respective medians
df_copy = df.copy()
med = df_copy[numeric_cols].median()
df_copy[numeric_cols] = df_copy[numeric_cols].fillna(med)

print(med.head(8))
print(df_copy[numeric_cols[:8]].head(5))
print(df[numeric_cols[:8]].head(5))

#impute non-bumeric columns with their most frequent values
most_freq = df_copy[non_numeric_cols].describe().loc['top'] #most common values for each non-numeric column
most_freq

#fill in the missing values with the most frequent values
df_copy[non_numeric_cols] = df_copy[non_numeric_cols].fillna(most_freq)

pct_missing_nonnum = df[non_numeric_cols].isna().mean()
pct_missing_nonnum
#for non-numeric columns, there are originally no missing values, so no imputation needed

"""#Outliers
##Identifying Outliers
"""

#use Kurtosis to detect outliers
df.kurt(numeric_only=True)[:10]

#we explore column life_sq further
df['life_sq'].describe()

#plot a histogram of the column life_sq

df['life_sq'].hist(bins=100)

#plot a boxplot of the column life_sq
df.boxplot(column=['life_sq'])

"""#Duplicates

##Identifying duplicates
"""

#dupliacted method grab the boolean values of whether a row is duplicated, and then use it to filtre for duplicated rows from the dataframe
df[df.duplicated()]

"""in case of duplicates, remove them by using drop_duplicates method.
df.drop_duplicates()
"""

#we can detect dupplicates based on a set of identifiers (columns).
# column id is unique. We drop it and check if there are any duplicates based on the rest of columns.
df[df.drop(columns=['id']).duplicated()]

#10 rows duplicated. so we remove them and save the new dataset as df_dedupped
df_dedupped = df.drop(columns=['id']).drop_duplicates()

#compare the shapes of the two datasets
print(df.shape)
print(df_dedupped.shape)

"""#Data Reduction
##Numerosity Reduction
"""

#Generate a simple random sample
#we specify the percentage of random rows to be extracted. in here we are extracting 25% of the dataframe
rand_sample = df_dedupped.sample(frac=0.25)

print(len(df_dedupped))
print(len(rand_sample))

#Print the random sample
rand_sample

#Extract 100 random rows
rand_sample_1 = df_dedupped.sample(n=100)
rand_sample_1

#Stratified Sampling
stratified_sample = df_dedupped.groupby('product_type').apply(lambda x: x.sample(frac=0.20))
stratified_sample

"""#Data Transformation
##Converting data types
"""

#convert to datetime format
df_dedupped['timestamp'] = pd.to_datetime(df_dedupped['timestamp'])

print(df.dtypes)

"""##Feature Construction"""

#extracting year
df_dedupped['year'] = df_dedupped['timestamp'].dt.year

print(df_dedupped[['timestamp','full_sq','life_sq','floor','price_doc','year']].head(5))

"""##Aggregation / Summarisation"""

#use groupby
output_agg = df_dedupped.groupby('year').agg({'year': 'count', 'price_doc': 'mean'})
output_agg = output_agg.rename(columns={'year': 'nbr_transactions', 'price_doc':'average_price'})
#Global change of number formats
def format_float(value):
  return f'{value:,.2f}'
pd.options.display.float_format = format_float
#print results
print(output_agg)